{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc09c66d-d3bc-4dcb-aaad-642eb0d2f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1966a9-5679-4677-9bfa-f79921bd4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandPoseClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandPoseClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 128)  # Input layer to first hidden layer\n",
    "        self.dropout1 = nn.Dropout(0.25)  # Dropout layer after the first hidden layer\n",
    "        self.fc2 = nn.Linear(128, 128)  # Second hidden layer\n",
    "        self.dropout2 = nn.Dropout(0.25)  # Dropout layer after the second hidden layer\n",
    "        self.fc3 = nn.Linear(128, 64)   # Third hidden layer\n",
    "        self.dropout3 = nn.Dropout(0.25)  # Dropout layer after the third hidden layer\n",
    "        self.fc4 = nn.Linear(64, 7)     # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Activation function for the first hidden layer\n",
    "        x = self.dropout1(x)     # Apply dropout\n",
    "        x = F.relu(self.fc2(x))  # Activation function for the second hidden layer\n",
    "        x = self.dropout2(x)     # Apply dropout\n",
    "        x = F.relu(self.fc3(x))  # Activation function for the third hidden layer\n",
    "        x = self.dropout3(x)     # Apply dropout\n",
    "        x = self.fc4(x)          # No activation function here for raw scores to go into the loss function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65342ce-c9ea-4152-abc6-554f3c67d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir(\"./data/\")\n",
    "\n",
    "class_indexes = []\n",
    "classes = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for fil in data_files:\n",
    "    parts = fil.split('_')\n",
    "    idx = int(parts[0])\n",
    "    class_name = parts[1][:-4]\n",
    "    classes.append(class_name)\n",
    "    class_indexes.append(idx)\n",
    "    df = pd.read_csv(\"./data/\" + fil)\n",
    "    data = df.iloc[:].values\n",
    "    one_hot = [0] * 7\n",
    "    one_hot[idx] = 1\n",
    "    for row in data:\n",
    "        X.append(row)\n",
    "        y.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08203715-2ae6-4bb3-89a7-34b81b17c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_spare, y_train, y_test_spare = train_test_split(X, y, test_size=0.2, shuffle = True)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_spare, y_test_spare, test_size=0.5, shuffle=True)\n",
    "\n",
    "X_train = torch.from_numpy(np.array(X_train).astype(np.float32))\n",
    "y_train = torch.from_numpy(np.array(y_train).astype(np.int64))\n",
    "\n",
    "X_test = torch.from_numpy(np.array(X_test).astype(np.float32))\n",
    "y_test = torch.from_numpy(np.array(y_test).astype(np.int64))\n",
    "\n",
    "X_val = torch.from_numpy(np.array(X_val).astype(np.float32))\n",
    "y_val = torch.from_numpy(np.array(y_val).astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c92b9e5-1bd4-4995-a966-6d6eb5d2c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        # Yield a tuple of the current batch's features and labels\n",
    "        yield X[i:i + batch_size], y[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91a7214-1861-4ec6-8ab2-3411cc4b6a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandPoseClassifier(\n",
      "  (fc1): Linear(in_features=63, out_features=128, bias=True)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout2): Dropout(p=0.25, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (dropout3): Dropout(p=0.25, inplace=False)\n",
      "  (fc4): Linear(in_features=64, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = HandPoseClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 200\n",
    "batch_size = 8\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1844c50-1513-46fd-a392-0db92b4394b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val)\n",
    "            val_loss = criterion(val_output, y_val)\n",
    "        \n",
    "        # Print loss every epoch or less frequently\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c656a4fc-720b-4101-b9d8-f11ac4d5eda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Training Loss: 1.947021245956421, Validation Loss: 1.9452214241027832\n",
      "Epoch 2/200, Training Loss: 1.9441438913345337, Validation Loss: 1.940927505493164\n",
      "Epoch 3/200, Training Loss: 1.9397190809249878, Validation Loss: 1.9364709854125977\n",
      "Epoch 4/200, Training Loss: 1.9371174573898315, Validation Loss: 1.9316926002502441\n",
      "Epoch 5/200, Training Loss: 1.9329721927642822, Validation Loss: 1.9262927770614624\n",
      "Epoch 6/200, Training Loss: 1.9286588430404663, Validation Loss: 1.920225739479065\n",
      "Epoch 7/200, Training Loss: 1.9248194694519043, Validation Loss: 1.913846492767334\n",
      "Epoch 8/200, Training Loss: 1.9190200567245483, Validation Loss: 1.906894326210022\n",
      "Epoch 9/200, Training Loss: 1.9133179187774658, Validation Loss: 1.899235486984253\n",
      "Epoch 10/200, Training Loss: 1.9077582359313965, Validation Loss: 1.890954613685608\n",
      "Epoch 11/200, Training Loss: 1.9018474817276, Validation Loss: 1.8818790912628174\n",
      "Epoch 12/200, Training Loss: 1.893531084060669, Validation Loss: 1.8717892169952393\n",
      "Epoch 13/200, Training Loss: 1.8855334520339966, Validation Loss: 1.8604916334152222\n",
      "Epoch 14/200, Training Loss: 1.8755271434783936, Validation Loss: 1.8478177785873413\n",
      "Epoch 15/200, Training Loss: 1.8654999732971191, Validation Loss: 1.83399498462677\n",
      "Epoch 16/200, Training Loss: 1.853745937347412, Validation Loss: 1.8190064430236816\n",
      "Epoch 17/200, Training Loss: 1.8396402597427368, Validation Loss: 1.8024396896362305\n",
      "Epoch 18/200, Training Loss: 1.8268530368804932, Validation Loss: 1.7842110395431519\n",
      "Epoch 19/200, Training Loss: 1.8120777606964111, Validation Loss: 1.764654517173767\n",
      "Epoch 20/200, Training Loss: 1.791523814201355, Validation Loss: 1.743401050567627\n",
      "Epoch 21/200, Training Loss: 1.7707959413528442, Validation Loss: 1.7202423810958862\n",
      "Epoch 22/200, Training Loss: 1.7519547939300537, Validation Loss: 1.6953625679016113\n",
      "Epoch 23/200, Training Loss: 1.7304434776306152, Validation Loss: 1.6687777042388916\n",
      "Epoch 24/200, Training Loss: 1.7090078592300415, Validation Loss: 1.6405351161956787\n",
      "Epoch 25/200, Training Loss: 1.6786967515945435, Validation Loss: 1.6105841398239136\n",
      "Epoch 26/200, Training Loss: 1.6530427932739258, Validation Loss: 1.579175591468811\n",
      "Epoch 27/200, Training Loss: 1.6264185905456543, Validation Loss: 1.5464471578598022\n",
      "Epoch 28/200, Training Loss: 1.5975335836410522, Validation Loss: 1.5127986669540405\n",
      "Epoch 29/200, Training Loss: 1.5641855001449585, Validation Loss: 1.4776380062103271\n",
      "Epoch 30/200, Training Loss: 1.5375858545303345, Validation Loss: 1.4406425952911377\n",
      "Epoch 31/200, Training Loss: 1.5071899890899658, Validation Loss: 1.4030969142913818\n",
      "Epoch 32/200, Training Loss: 1.4792864322662354, Validation Loss: 1.3654897212982178\n",
      "Epoch 33/200, Training Loss: 1.4385461807250977, Validation Loss: 1.3275096416473389\n",
      "Epoch 34/200, Training Loss: 1.409544825553894, Validation Loss: 1.2881852388381958\n",
      "Epoch 35/200, Training Loss: 1.3699051141738892, Validation Loss: 1.2490638494491577\n",
      "Epoch 36/200, Training Loss: 1.3437292575836182, Validation Loss: 1.2108592987060547\n",
      "Epoch 37/200, Training Loss: 1.3075679540634155, Validation Loss: 1.1736449003219604\n",
      "Epoch 38/200, Training Loss: 1.2697592973709106, Validation Loss: 1.1329355239868164\n",
      "Epoch 39/200, Training Loss: 1.2398045063018799, Validation Loss: 1.094848394393921\n",
      "Epoch 40/200, Training Loss: 1.2176676988601685, Validation Loss: 1.0589838027954102\n",
      "Epoch 41/200, Training Loss: 1.1728178262710571, Validation Loss: 1.0221596956253052\n",
      "Epoch 42/200, Training Loss: 1.1496161222457886, Validation Loss: 0.9853143692016602\n",
      "Epoch 43/200, Training Loss: 1.1076903343200684, Validation Loss: 0.9490162134170532\n",
      "Epoch 44/200, Training Loss: 1.0839635133743286, Validation Loss: 0.9137740731239319\n",
      "Epoch 45/200, Training Loss: 1.0482096672058105, Validation Loss: 0.8794382214546204\n",
      "Epoch 46/200, Training Loss: 1.0160619020462036, Validation Loss: 0.8469458818435669\n",
      "Epoch 47/200, Training Loss: 0.9940677285194397, Validation Loss: 0.8151347041130066\n",
      "Epoch 48/200, Training Loss: 0.96322101354599, Validation Loss: 0.7824548482894897\n",
      "Epoch 49/200, Training Loss: 0.9210782051086426, Validation Loss: 0.7512454986572266\n",
      "Epoch 50/200, Training Loss: 0.9000620245933533, Validation Loss: 0.7207692265510559\n",
      "Epoch 51/200, Training Loss: 0.8801795244216919, Validation Loss: 0.69203782081604\n",
      "Epoch 52/200, Training Loss: 0.8436137437820435, Validation Loss: 0.6649059653282166\n",
      "Epoch 53/200, Training Loss: 0.8247336745262146, Validation Loss: 0.6383647322654724\n",
      "Epoch 54/200, Training Loss: 0.7933022379875183, Validation Loss: 0.6119621992111206\n",
      "Epoch 55/200, Training Loss: 0.7743992805480957, Validation Loss: 0.5887834429740906\n",
      "Epoch 56/200, Training Loss: 0.7510040998458862, Validation Loss: 0.567537784576416\n",
      "Epoch 57/200, Training Loss: 0.7258834838867188, Validation Loss: 0.5456814169883728\n",
      "Epoch 58/200, Training Loss: 0.7066535949707031, Validation Loss: 0.5241178870201111\n",
      "Epoch 59/200, Training Loss: 0.6904757022857666, Validation Loss: 0.5047457814216614\n",
      "Epoch 60/200, Training Loss: 0.6705911159515381, Validation Loss: 0.48734453320503235\n",
      "Epoch 61/200, Training Loss: 0.6418607831001282, Validation Loss: 0.4676196873188019\n",
      "Epoch 62/200, Training Loss: 0.6319693922996521, Validation Loss: 0.45005398988723755\n",
      "Epoch 63/200, Training Loss: 0.6035895943641663, Validation Loss: 0.43371501564979553\n",
      "Epoch 64/200, Training Loss: 0.5881234407424927, Validation Loss: 0.41847464442253113\n",
      "Epoch 65/200, Training Loss: 0.5733161568641663, Validation Loss: 0.4048226475715637\n",
      "Epoch 66/200, Training Loss: 0.5474135875701904, Validation Loss: 0.39100268483161926\n",
      "Epoch 67/200, Training Loss: 0.5474047064781189, Validation Loss: 0.37686944007873535\n",
      "Epoch 68/200, Training Loss: 0.5367524027824402, Validation Loss: 0.3636353313922882\n",
      "Epoch 69/200, Training Loss: 0.5134789943695068, Validation Loss: 0.3516624867916107\n",
      "Epoch 70/200, Training Loss: 0.5034053921699524, Validation Loss: 0.3384759724140167\n",
      "Epoch 71/200, Training Loss: 0.49281883239746094, Validation Loss: 0.3274739384651184\n",
      "Epoch 72/200, Training Loss: 0.4807347059249878, Validation Loss: 0.3175831437110901\n",
      "Epoch 73/200, Training Loss: 0.46174827218055725, Validation Loss: 0.3094279170036316\n",
      "Epoch 74/200, Training Loss: 0.45461222529411316, Validation Loss: 0.300910621881485\n",
      "Epoch 75/200, Training Loss: 0.4416257441043854, Validation Loss: 0.29299822449684143\n",
      "Epoch 76/200, Training Loss: 0.43317610025405884, Validation Loss: 0.2842266857624054\n",
      "Epoch 77/200, Training Loss: 0.42993390560150146, Validation Loss: 0.27801188826560974\n",
      "Epoch 78/200, Training Loss: 0.4144483208656311, Validation Loss: 0.27179113030433655\n",
      "Epoch 79/200, Training Loss: 0.4089214503765106, Validation Loss: 0.26462996006011963\n",
      "Epoch 80/200, Training Loss: 0.3938659429550171, Validation Loss: 0.2589319944381714\n",
      "Epoch 81/200, Training Loss: 0.3911738693714142, Validation Loss: 0.2530447542667389\n",
      "Epoch 82/200, Training Loss: 0.3878779709339142, Validation Loss: 0.2481851875782013\n",
      "Epoch 83/200, Training Loss: 0.37998953461647034, Validation Loss: 0.2419157177209854\n",
      "Epoch 84/200, Training Loss: 0.3673502504825592, Validation Loss: 0.23719632625579834\n",
      "Epoch 85/200, Training Loss: 0.35720065236091614, Validation Loss: 0.2307453453540802\n",
      "Epoch 86/200, Training Loss: 0.3549668788909912, Validation Loss: 0.22764727473258972\n",
      "Epoch 87/200, Training Loss: 0.349478542804718, Validation Loss: 0.22046902775764465\n",
      "Epoch 88/200, Training Loss: 0.34384647011756897, Validation Loss: 0.2159399539232254\n",
      "Epoch 89/200, Training Loss: 0.3360215127468109, Validation Loss: 0.21145130693912506\n",
      "Epoch 90/200, Training Loss: 0.3276565670967102, Validation Loss: 0.20830625295639038\n",
      "Epoch 91/200, Training Loss: 0.31290823221206665, Validation Loss: 0.20355916023254395\n",
      "Epoch 92/200, Training Loss: 0.3164985179901123, Validation Loss: 0.1977292001247406\n",
      "Epoch 93/200, Training Loss: 0.30420491099357605, Validation Loss: 0.1932636797428131\n",
      "Epoch 94/200, Training Loss: 0.3086480498313904, Validation Loss: 0.18913666903972626\n",
      "Epoch 95/200, Training Loss: 0.3084027171134949, Validation Loss: 0.18722768127918243\n",
      "Epoch 96/200, Training Loss: 0.30115288496017456, Validation Loss: 0.1849043220281601\n",
      "Epoch 97/200, Training Loss: 0.29085105657577515, Validation Loss: 0.18093277513980865\n",
      "Epoch 98/200, Training Loss: 0.29398515820503235, Validation Loss: 0.1776030957698822\n",
      "Epoch 99/200, Training Loss: 0.2776821255683899, Validation Loss: 0.17388540506362915\n",
      "Epoch 100/200, Training Loss: 0.282723993062973, Validation Loss: 0.17113536596298218\n",
      "Epoch 101/200, Training Loss: 0.2692866921424866, Validation Loss: 0.1676936149597168\n",
      "Epoch 102/200, Training Loss: 0.2685776948928833, Validation Loss: 0.164159893989563\n",
      "Epoch 103/200, Training Loss: 0.26395148038864136, Validation Loss: 0.16155299544334412\n",
      "Epoch 104/200, Training Loss: 0.262749582529068, Validation Loss: 0.15970972180366516\n",
      "Epoch 105/200, Training Loss: 0.2589316666126251, Validation Loss: 0.15765047073364258\n",
      "Epoch 106/200, Training Loss: 0.2575462758541107, Validation Loss: 0.1555570513010025\n",
      "Epoch 107/200, Training Loss: 0.2511649429798126, Validation Loss: 0.15066953003406525\n",
      "Epoch 108/200, Training Loss: 0.24154171347618103, Validation Loss: 0.14850474894046783\n",
      "Epoch 109/200, Training Loss: 0.2544819414615631, Validation Loss: 0.14597436785697937\n",
      "Epoch 110/200, Training Loss: 0.2329535037279129, Validation Loss: 0.14665402472019196\n",
      "Epoch 111/200, Training Loss: 0.23756439983844757, Validation Loss: 0.14449992775917053\n",
      "Epoch 112/200, Training Loss: 0.23443645238876343, Validation Loss: 0.14228114485740662\n",
      "Epoch 113/200, Training Loss: 0.23819293081760406, Validation Loss: 0.13921336829662323\n",
      "Epoch 114/200, Training Loss: 0.22828935086727142, Validation Loss: 0.1383877694606781\n",
      "Epoch 115/200, Training Loss: 0.2246813029050827, Validation Loss: 0.13800667226314545\n",
      "Epoch 116/200, Training Loss: 0.22199726104736328, Validation Loss: 0.13280056416988373\n",
      "Epoch 117/200, Training Loss: 0.21908390522003174, Validation Loss: 0.13196586072444916\n",
      "Epoch 118/200, Training Loss: 0.22033126652240753, Validation Loss: 0.12878955900669098\n",
      "Epoch 119/200, Training Loss: 0.20919254422187805, Validation Loss: 0.12954525649547577\n",
      "Epoch 120/200, Training Loss: 0.21022726595401764, Validation Loss: 0.12893573939800262\n",
      "Epoch 121/200, Training Loss: 0.21280765533447266, Validation Loss: 0.1258946657180786\n",
      "Epoch 122/200, Training Loss: 0.20284466445446014, Validation Loss: 0.1244853064417839\n",
      "Epoch 123/200, Training Loss: 0.20056934654712677, Validation Loss: 0.12117866426706314\n",
      "Epoch 124/200, Training Loss: 0.20578166842460632, Validation Loss: 0.11986764520406723\n",
      "Epoch 125/200, Training Loss: 0.19517509639263153, Validation Loss: 0.12022408843040466\n",
      "Epoch 126/200, Training Loss: 0.1987818330526352, Validation Loss: 0.11723488569259644\n",
      "Epoch 127/200, Training Loss: 0.19729803502559662, Validation Loss: 0.11642222851514816\n",
      "Epoch 128/200, Training Loss: 0.1916102021932602, Validation Loss: 0.11475526541471481\n",
      "Epoch 129/200, Training Loss: 0.1913572996854782, Validation Loss: 0.11395694315433502\n",
      "Epoch 130/200, Training Loss: 0.1794007569551468, Validation Loss: 0.11344274133443832\n",
      "Epoch 131/200, Training Loss: 0.1799580603837967, Validation Loss: 0.11121603846549988\n",
      "Epoch 132/200, Training Loss: 0.18504805862903595, Validation Loss: 0.10844206809997559\n",
      "Epoch 133/200, Training Loss: 0.18497246503829956, Validation Loss: 0.1071363091468811\n",
      "Epoch 134/200, Training Loss: 0.17636583745479584, Validation Loss: 0.10575199127197266\n",
      "Epoch 135/200, Training Loss: 0.17963095009326935, Validation Loss: 0.10497432202100754\n",
      "Epoch 136/200, Training Loss: 0.17475628852844238, Validation Loss: 0.10539746284484863\n",
      "Epoch 137/200, Training Loss: 0.1779564917087555, Validation Loss: 0.10385487228631973\n",
      "Epoch 138/200, Training Loss: 0.1750173419713974, Validation Loss: 0.10227805376052856\n",
      "Epoch 139/200, Training Loss: 0.17231385409832, Validation Loss: 0.10054267942905426\n",
      "Epoch 140/200, Training Loss: 0.1660584956407547, Validation Loss: 0.09917600452899933\n",
      "Epoch 141/200, Training Loss: 0.16927608847618103, Validation Loss: 0.09830036014318466\n",
      "Epoch 142/200, Training Loss: 0.16481401026248932, Validation Loss: 0.09795162826776505\n",
      "Epoch 143/200, Training Loss: 0.16170236468315125, Validation Loss: 0.09678135812282562\n",
      "Epoch 144/200, Training Loss: 0.16577096283435822, Validation Loss: 0.0956190675497055\n",
      "Epoch 145/200, Training Loss: 0.15339535474777222, Validation Loss: 0.09415172040462494\n",
      "Epoch 146/200, Training Loss: 0.15532158315181732, Validation Loss: 0.09283151477575302\n",
      "Epoch 147/200, Training Loss: 0.15099844336509705, Validation Loss: 0.09222788363695145\n",
      "Epoch 148/200, Training Loss: 0.14741256833076477, Validation Loss: 0.09107790887355804\n",
      "Epoch 149/200, Training Loss: 0.15269282460212708, Validation Loss: 0.089903324842453\n",
      "Epoch 150/200, Training Loss: 0.15026050806045532, Validation Loss: 0.08866716176271439\n",
      "Epoch 151/200, Training Loss: 0.14523111283779144, Validation Loss: 0.08731070905923843\n",
      "Epoch 152/200, Training Loss: 0.14702612161636353, Validation Loss: 0.0861145630478859\n",
      "Epoch 153/200, Training Loss: 0.14584672451019287, Validation Loss: 0.08595302700996399\n",
      "Epoch 154/200, Training Loss: 0.14137789607048035, Validation Loss: 0.08487193286418915\n",
      "Epoch 155/200, Training Loss: 0.14154724776744843, Validation Loss: 0.08354464918375015\n",
      "Epoch 156/200, Training Loss: 0.13339921832084656, Validation Loss: 0.08221131563186646\n",
      "Epoch 157/200, Training Loss: 0.13873395323753357, Validation Loss: 0.08159228414297104\n",
      "Epoch 158/200, Training Loss: 0.13871125876903534, Validation Loss: 0.08122753351926804\n",
      "Epoch 159/200, Training Loss: 0.1348506212234497, Validation Loss: 0.08045144379138947\n",
      "Epoch 160/200, Training Loss: 0.138096883893013, Validation Loss: 0.07955199480056763\n",
      "Epoch 161/200, Training Loss: 0.13629506528377533, Validation Loss: 0.07830233126878738\n",
      "Epoch 162/200, Training Loss: 0.12663792073726654, Validation Loss: 0.07692589610815048\n",
      "Epoch 163/200, Training Loss: 0.13049300014972687, Validation Loss: 0.07615136355161667\n",
      "Epoch 164/200, Training Loss: 0.13168714940547943, Validation Loss: 0.0752464309334755\n",
      "Epoch 165/200, Training Loss: 0.13197080790996552, Validation Loss: 0.07482511550188065\n",
      "Epoch 166/200, Training Loss: 0.11858287453651428, Validation Loss: 0.07398036867380142\n",
      "Epoch 167/200, Training Loss: 0.12704679369926453, Validation Loss: 0.07318008691072464\n",
      "Epoch 168/200, Training Loss: 0.12107667326927185, Validation Loss: 0.07238543033599854\n",
      "Epoch 169/200, Training Loss: 0.12568965554237366, Validation Loss: 0.07155106961727142\n",
      "Epoch 170/200, Training Loss: 0.12159931659698486, Validation Loss: 0.07076170295476913\n",
      "Epoch 171/200, Training Loss: 0.12085135281085968, Validation Loss: 0.07013288885354996\n",
      "Epoch 172/200, Training Loss: 0.12081947922706604, Validation Loss: 0.06990981847047806\n",
      "Epoch 173/200, Training Loss: 0.1151726171374321, Validation Loss: 0.06924611330032349\n",
      "Epoch 174/200, Training Loss: 0.11511225998401642, Validation Loss: 0.06721869856119156\n",
      "Epoch 175/200, Training Loss: 0.12048211693763733, Validation Loss: 0.06568123400211334\n",
      "Epoch 176/200, Training Loss: 0.11693780869245529, Validation Loss: 0.06505493819713593\n",
      "Epoch 177/200, Training Loss: 0.11501204967498779, Validation Loss: 0.06522483378648758\n",
      "Epoch 178/200, Training Loss: 0.11319062858819962, Validation Loss: 0.06703433394432068\n",
      "Epoch 179/200, Training Loss: 0.10644121468067169, Validation Loss: 0.0670824721455574\n",
      "Epoch 180/200, Training Loss: 0.11487951874732971, Validation Loss: 0.06365400552749634\n",
      "Epoch 181/200, Training Loss: 0.1086762547492981, Validation Loss: 0.06178619712591171\n",
      "Epoch 182/200, Training Loss: 0.1125260442495346, Validation Loss: 0.061344895511865616\n",
      "Epoch 183/200, Training Loss: 0.11151638627052307, Validation Loss: 0.06106794252991676\n",
      "Epoch 184/200, Training Loss: 0.10281241685152054, Validation Loss: 0.06047748401761055\n",
      "Epoch 185/200, Training Loss: 0.09979379922151566, Validation Loss: 0.059541966766119\n",
      "Epoch 186/200, Training Loss: 0.10114571452140808, Validation Loss: 0.05811880901455879\n",
      "Epoch 187/200, Training Loss: 0.1060699000954628, Validation Loss: 0.0571860633790493\n",
      "Epoch 188/200, Training Loss: 0.10673179477453232, Validation Loss: 0.05732768774032593\n",
      "Epoch 189/200, Training Loss: 0.09947795420885086, Validation Loss: 0.05725550651550293\n",
      "Epoch 190/200, Training Loss: 0.10175879299640656, Validation Loss: 0.05653032660484314\n",
      "Epoch 191/200, Training Loss: 0.09781482070684433, Validation Loss: 0.05555320158600807\n",
      "Epoch 192/200, Training Loss: 0.09728297591209412, Validation Loss: 0.05475635454058647\n",
      "Epoch 193/200, Training Loss: 0.09372606873512268, Validation Loss: 0.054035935550928116\n",
      "Epoch 194/200, Training Loss: 0.10006851702928543, Validation Loss: 0.05343043431639671\n",
      "Epoch 195/200, Training Loss: 0.09407155960798264, Validation Loss: 0.05365389212965965\n",
      "Epoch 196/200, Training Loss: 0.09525477141141891, Validation Loss: 0.054009828716516495\n",
      "Epoch 197/200, Training Loss: 0.09067105501890182, Validation Loss: 0.05338825657963753\n",
      "Epoch 198/200, Training Loss: 0.09023340791463852, Validation Loss: 0.05198782682418823\n",
      "Epoch 199/200, Training Loss: 0.09005124866962433, Validation Loss: 0.05145435780286789\n",
      "Epoch 200/200, Training Loss: 0.0925774946808815, Validation Loss: 0.05053886026144028\n"
     ]
    }
   ],
   "source": [
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca9d8d6a-0ed9-49f9-ad2f-6a75992a6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce3462-5fcf-4c83-8367-4df1390cc43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
